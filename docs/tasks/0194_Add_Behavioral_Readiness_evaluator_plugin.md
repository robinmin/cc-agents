---
name: Add Behavioral Readiness evaluator plugin
description: Create evaluator plugin that scores behavioral readiness - test scenario coverage, edge case handling, error recovery patterns
status: Completed
created_at: 2026-02-11 15:49:01
updated_at: 2026-02-11 18:01:39
impl_progress:
  planning: completed
  design: completed
  implementation: completed
  review: completed
  testing: completed
  planning: pending
  design: pending
  implementation: pending
  review: pending
  testing: pending
---

## 0194. Add Behavioral Readiness evaluator plugin

### Background

Part of task 0190 (Phase 1). Current evaluation is purely static - it can't tell if a skill will actually improve agent behavior. This evaluator checks for "behavioral readiness" indicators: does the skill include test scenarios, edge case handling, error recovery patterns, and anti-patterns? Per Anthropic's eval framework: "Convert user-reported failures into test cases" and "Write unambiguous tasks with reference solutions." This dimension rewards skills that are testable.

Target file: `plugins/rd2/skills/cc-skills/scripts/evaluators/behavioral_readiness.py`
Weight: 10%

### Requirements

- [ ] Create `behavioral_readiness.py` evaluator implementing `DimensionEvaluator` protocol
- [ ] Check for example/scenario sections with concrete inputs and expected outputs
- [ ] Detect anti-pattern documentation ("Common Mistakes", "What NOT to do", "Don't")
- [ ] Score error handling guidance: does the skill tell the agent what to do when things go wrong?
- [ ] Check for edge case coverage: does the skill mention boundary conditions?
- [ ] Bonus for `tests/` directory or scenario files
- [ ] Check for positive + negative examples (both "do this" and "don't do this")
- [ ] Use rubric-based scoring
- [ ] All existing tests pass
- [ ] New unit tests for the evaluator

### Q&A

[Clarifications added during planning phase]

### Design

[Architecture/UI specs added by specialists]

### Solution


Approach: Create a new `behavioral_readiness.py` evaluator plugin that assesses whether a skill is ready for real-world behavioral use by Claude. This evaluator checks for error handling guidance, edge case coverage, fallback strategies, verification steps, and output format specifications. It follows the existing DimensionEvaluator protocol pattern.

Key decisions:
- Evaluator checks: error handling presence, edge case documentation, fallback strategies, verification/validation steps, output format clarity
- Scoring criteria: error handling (25%), edge case coverage (25%), fallback strategies (20%), verification steps (15%), output format specs (15%)
- Analyze SKILL.md for behavioral readiness patterns (error keywords, edge case sections, fallback logic)
- Return structured findings with score, issues list, and suggestions

Files to create/modify:
- `plugins/rd2/skills/cc-skills/scripts/evaluators/behavioral_readiness.py` -- New evaluator implementing DimensionEvaluator protocol
- `plugins/rd2/skills/cc-skills/scripts/evaluators/__init__.py` -- Add BehavioralReadinessEvaluator to exports
- `plugins/rd2/skills/cc-skills/tests/test_behavioral_readiness.py` -- Unit tests

Acceptance criteria:
- Implements DimensionEvaluator protocol (name, weight, evaluate methods)
- Correctly scores skills based on behavioral readiness
- Detects missing error handling, edge cases, fallbacks
- Exports from __init__.py for auto-discovery
- Unit tests pass
- Existing tests unaffected

### Plan

[Step-by-step implementation plan]

### Artifacts

| Type | Path | Generated By | Date |
| ---- | ---- | ------------ | ---- |

### References

- Parent: docs/tasks/0190_enhance_Agent_Skills_cc-skills.md
- Brainstorm: docs/.tasks/brainstorm/0190_brainstorm.md
- Anthropic eval framework: https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents

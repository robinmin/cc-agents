---
name: Add Value-Add Assessment evaluator plugin
description: Create evaluator plugin that assesses whether skill adds knowledge beyond Claude's base capabilities - overlap detection, unique value scoring
status: Completed
created_at: 2026-02-11 15:49:01
updated_at: 2026-02-11 18:01:39
impl_progress:
  planning: completed
  design: completed
  implementation: completed
  review: completed
  testing: completed
  planning: pending
  design: pending
  implementation: pending
  review: pending
  testing: pending
---

## 0193. Add Value-Add Assessment evaluator plugin

### Background

Part of task 0190 (Phase 1). A key gap: current evaluation doesn't assess whether a skill teaches Claude something it doesn't already know. Per the evaluation-first methodology already in cc-skills: "Write content based on observed failure modes, not anticipated needs." Per Kotrotsos: "If the agent could skip this skill and still succeed, the skill is probably too broad." This evaluator checks for common low-value patterns.

Target file: `plugins/rd2/skills/cc-skills/scripts/evaluators/value_add.py`
Weight: 10%

### Requirements

- [ ] Create `value_add.py` evaluator implementing `DimensionEvaluator` protocol
- [ ] Detect generic content patterns: explaining well-known concepts (REST, SQL, HTTP), restating language docs, general programming advice
- [ ] Score specificity: ratio of project-specific/domain-specific content vs. generic content
- [ ] Check for concrete artifacts: scripts, templates, schemas, reference data that add genuine value
- [ ] Detect "wrapper-only" skills that just restate what Claude already knows
- [ ] Positive scoring for: custom tools, proprietary workflows, project-specific patterns, error-specific guidance
- [ ] Use rubric-based scoring
- [ ] All existing tests pass
- [ ] New unit tests for the evaluator

### Q&A

[Clarifications added during planning phase]

### Design

[Architecture/UI specs added by specialists]

### Solution


Approach: Create a new `value_add.py` evaluator plugin that assesses whether a skill provides genuine value beyond what Claude can do natively. This evaluator checks for domain-specific knowledge, unique workflows, specialized techniques, and differentiation from generic prompting. It follows the existing DimensionEvaluator protocol pattern.

Key decisions:
- Evaluator checks: domain-specific content presence, unique workflow definition, specialized technique documentation, differentiation from generic capabilities
- Scoring criteria: domain expertise depth (30%), workflow uniqueness (25%), specialized techniques (25%), generic overlap avoidance (20%)
- Analyze SKILL.md for domain-specific terminology density, unique procedural steps, and specialized references
- Return structured findings with score, issues list, and suggestions

Files to create/modify:
- `plugins/rd2/skills/cc-skills/scripts/evaluators/value_add.py` -- New evaluator implementing DimensionEvaluator protocol
- `plugins/rd2/skills/cc-skills/scripts/evaluators/__init__.py` -- Add ValueAddEvaluator to exports
- `plugins/rd2/skills/cc-skills/tests/test_value_add.py` -- Unit tests

Acceptance criteria:
- Implements DimensionEvaluator protocol (name, weight, evaluate methods)
- Correctly scores skills based on value-add assessment
- Detects generic/low-value content vs domain-specific expertise
- Exports from __init__.py for auto-discovery
- Unit tests pass
- Existing tests unaffected

### Plan

[Step-by-step implementation plan]

### Artifacts

| Type | Path | Generated By | Date |
| ---- | ---- | ------------ | ---- |

### References

- Parent: docs/tasks/0190_enhance_Agent_Skills_cc-skills.md
- Brainstorm: docs/.tasks/brainstorm/0190_brainstorm.md
- Evaluation-first methodology: plugins/rd2/skills/cc-skills/references/evaluation.md

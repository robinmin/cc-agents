---
name: Add LLM-as-Judge evaluation with --deep flag
description: Add --deep flag to evaluate command enabling LLM-as-Judge grading for subjective dimensions with structured rubrics and calibration
status: Completed
created_at: 2026-02-11 15:49:01
updated_at: 2026-02-11 16:30:00
impl_progress:
  planning: completed
  design: completed
  implementation: completed
  review: completed
  testing: completed
---

## 0196. Add LLM-as-Judge evaluation with --deep flag

### Background

Part of task 0190 (Phase 2). Some evaluation dimensions are inherently subjective and can't be fully captured by static analysis - e.g., "Is this instruction clear?" or "Does this skill add value?" Per Anthropic's eval framework, effective evaluation combines "code-based evaluators" (fast, objective) with "model-based graders" (nuanced, calibratable). LLM-as-Judge uses structured rubrics to score subjective dimensions, calibrated against human expert judgment.

Target files: `plugins/rd2/skills/cc-skills/scripts/evaluators/llm_judge.py` (base), rubric files in `references/rubrics/`

### Requirements

- [ ] Create `llm_judge.py` base class for LLM-based evaluation
- [ ] Add `--deep` flag to `cmd_evaluate` that enables LLM-based grading alongside static analysis
- [ ] Implement LLM grading for at least 2 dimensions: instruction clarity and value-add assessment
- [ ] Define structured rubrics (YAML) with 5-level scales and explicit criteria per level
- [ ] Include "Unknown" escape option per Anthropic's recommendation
- [ ] Support pass@k metrics: run evaluation k times, report consistency
- [ ] Graceful fallback when API unavailable (skip LLM dimensions, report static-only results)
- [ ] Cost reporting: show token usage and estimated cost per evaluation
- [ ] All existing tests pass
- [ ] New unit tests (mock API calls)

### Q&A

[Clarifications added during planning phase]

### Design

[Architecture/UI specs added by specialists]

### Solution

[Solution added by specialists]

### Plan

[Step-by-step implementation plan]

### Artifacts

| Type | Path | Generated By | Date |
| ---- | ---- | ------------ | ---- |

### References

- Parent: docs/tasks/0190_enhance_Agent_Skills_cc-skills.md
- Brainstorm: docs/.tasks/brainstorm/0190_brainstorm.md
- Anthropic eval framework: https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents
- Depends on: 0195 (rubric format must be defined first)

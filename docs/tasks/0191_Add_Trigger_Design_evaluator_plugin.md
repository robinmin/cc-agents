---
name: Add Trigger Design evaluator plugin
description: Create evaluator plugin that scores skill trigger/discovery quality - CSO optimization, keyword coverage, false positive/negative analysis
status: Completed
created_at: 2026-02-11 15:48:54
updated_at: 2026-02-11 18:01:38
impl_progress:
  planning: completed
  design: completed
  implementation: completed
  review: completed
  testing: completed
  planning: pending
  design: pending
  implementation: pending
  review: pending
  testing: pending
---

## 0191. Add Trigger Design evaluator plugin

### Background

Part of task 0190 (Phase 1). The current cc-skills evaluation has no dimension for assessing how well a skill will be discovered and triggered by Claude. A skill with perfect content but a vague description will never fire. Per Kotrotsos: "If the agent could skip this skill and still succeed, the skill is probably too broad." Per Anthropic's eval framework: test both positive cases (should trigger) and negative cases (shouldn't trigger).

Target file: `plugins/rd2/skills/cc-skills/scripts/evaluators/trigger_design.py`
Weight: 15%

### Requirements

- [ ] Create `trigger_design.py` evaluator implementing `DimensionEvaluator` protocol
- [ ] Score description field for: specific trigger phrases (quoted strings), third-person form, keyword coverage (synonyms, error messages, tool names)
- [ ] Detect anti-patterns: vague descriptions, workflow summaries in description (CSO violation), missing "when to use" signals
- [ ] Count unique trigger phrases in description (target: 3+ quoted phrases)
- [ ] Check for synonym coverage (e.g., if skill handles "timeout", does description also cover "hang", "freeze"?)
- [ ] Use rubric-based scoring (not point deductions)
- [ ] All existing tests pass
- [ ] New unit tests for the evaluator

### Q&A

[Clarifications added during planning phase]

### Design

[Architecture/UI specs added by specialists]

### Solution


Approach: Create a new `trigger_design.py` evaluator plugin that assesses how well a skill defines its activation triggers. This evaluator checks for the presence and quality of trigger phrases, "when to use" documentation, keyword signals, and disambiguation guidance. It follows the existing DimensionEvaluator protocol pattern used by other evaluators in the same directory.

Key decisions:
- Evaluator checks: trigger phrase presence in description, "when to use" section, keyword variety, disambiguation from similar skills
- Scoring criteria: description trigger phrases (30%), when-to-use documentation (25%), keyword signal coverage (25%), disambiguation clarity (20%)
- Use AST-free text analysis (read SKILL.md content, parse sections, check for patterns)
- Return structured findings with score, issues list, and suggestions

Files to create/modify:
- `plugins/rd2/skills/cc-skills/scripts/evaluators/trigger_design.py` -- New evaluator implementing DimensionEvaluator protocol
- `plugins/rd2/skills/cc-skills/scripts/evaluators/__init__.py` -- Add TriggerDesignEvaluator to exports
- `plugins/rd2/skills/cc-skills/tests/test_trigger_design.py` -- Unit tests

Acceptance criteria:
- Implements DimensionEvaluator protocol (name, weight, evaluate methods)
- Correctly scores skills based on trigger design quality
- Exports from __init__.py for auto-discovery
- Unit tests pass
- Existing tests unaffected

### Plan

[Step-by-step implementation plan]

### Artifacts

| Type | Path | Generated By | Date |
| ---- | ---- | ------------ | ---- |

### References

- Parent: docs/tasks/0190_enhance_Agent_Skills_cc-skills.md
- Brainstorm: docs/.tasks/brainstorm/0190_brainstorm.md
- Existing evaluators: plugins/rd2/skills/cc-skills/scripts/evaluators/
- CSO section in SKILL.md: plugins/rd2/skills/cc-skills/SKILL.md (line ~726)
